{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gym and other rl frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gym](https://github.com/openai/gym/) provides a set of environments to test reinforcement learning algorithms. In this quick intro we will discuss:\n",
    "\n",
    "* [How to use it](#1-how-to-use-openai-gym)\n",
    "* [How to create our own wrappers](#2-creating-gym-environment-wrappers)\n",
    "<!--* [How to use some other environments from different libraries](#3-using-other-environments)-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some other functionality we might need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "# some fixes required to run some environments\n",
    "def loadDynamicDeps():\n",
    "    \"\"\"Hacky-fix to load the appropiate GLEW library\n",
    "\n",
    "    Based on this fixes from controlsuite repo:\n",
    "    https://github.com/deepmind/dm_control/blob/978230f1376de1826c430dd3dfc0e3c7f742f5fe/dm_control/mujoco/wrapper/util.py#L100\n",
    "    \"\"\"\n",
    "    import ctypes\n",
    "\n",
    "    _libGlewLibraryPath = ctypes.util.find_library( 'GLEW' )\n",
    "    ctypes.CDLL( _libGlewLibraryPath, ctypes.RTLD_GLOBAL )\n",
    "    \n",
    "loadDynamicDeps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How to use OpenAI Gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a single-line import does the trick\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gym provides various environments, each accessible through its own environment-id. The list can be found [here](https://github.com/openai/gym/blob/master/docs/environments.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and environment through its ID\n",
    "env = gym.make( 'CartPole-v0' )\n",
    "# inspect it a little bit\n",
    "print( 'type: %s' % type(env) )\n",
    "print( 'components: \\n\\r %s' % str( dir( env ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of an environment\n",
    "\n",
    "Let's check some of the main components of the environment interface provided by gym. This implements most\n",
    "of the agent-environment interface discusses in the slides, as shown in the figure below:\n",
    "\n",
    "<img src='./imgs/img_rl_loop.png' width='50%'/>\n",
    "\n",
    "#### State-space $S=\\lbrace s \\rbrace$ \n",
    "\n",
    "This is defined by the `observation_space` attribute of the `env` wrapper, which can be either a `gym.spaces.Discrete` or a `gym.spaces.Box`, for **discrete** and **continuous** state/observation spaces respectively.\n",
    "\n",
    "#### Action-space $A=\\lbrace a \\rbrace$ \n",
    "\n",
    "This one is defined by the `action_space` attribute of the `env` wrapper, which can be also either a `gym.spaces.Discrete` or a `gym.spaces.Box`, for **discrete** and **continuous** state/observation spaces respectively.\n",
    "\n",
    "#### Initial state-distribution $\\rho_{0}(s)$\n",
    "\n",
    "We don't have full access to this distribution. Instead, it's already encoded in each environment implementation and\n",
    "by calling the `env.reset` method we get a sample from this distribution.\n",
    "\n",
    "```python\n",
    "# sample initial state from initial state distribution\n",
    "state = env.reset()\n",
    "```\n",
    "\n",
    "### Reward function $R(s,a,s')$ and Transition model $p(s'|s,a)$\n",
    "\n",
    "We don't have complete access to these two. Instead, we only have limited access via samples taken by\n",
    "stepping on the environment. These samples can be obtained with the `env.step` method, which accepts an\n",
    "action to be taken in the environment.\n",
    "\n",
    "```python\n",
    "# decide some action to take in state 'state'\n",
    "action = some_policy( state )\n",
    "# take a step in the environment (samples from transition model and reward function)\n",
    "next_state, reward, done, info = env.step( action )\n",
    "```\n",
    "\n",
    "### Seeding\n",
    "\n",
    "Recall that most of the components from before are probability distributions, which in the core are implemented using some random generators or similar. We can tell the environment to use a specific seed such that we could later reproduce\n",
    "our experiments or see variability over different random seeds. Just use the `env.seed` method, and pass an integer as seed.\n",
    "\n",
    "```python\n",
    "# create a registered environment\n",
    "env = gym.make( 'SOME-TOTALLY-AWESOME-ENVIRONMENT' )\n",
    "# seed the environment\n",
    "env.seed( 0 )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##---------- Let's play a bit with those components ----------------\n",
    "\n",
    "def runner( env_name ) :\n",
    "    # create the environment given the name\n",
    "    env = gym.make( env_name )\n",
    "    # grab some information about the environment type\n",
    "    _isObsContinuous = isinstance( env.observation_space, gym.spaces.Box )\n",
    "    _isActContinuous = isinstance( env.action_space, gym.spaces.Box )\n",
    "    \n",
    "    print( 'Environment: %s' % env_name )\n",
    "    print( 'observation-space: ', env.observation_space )\n",
    "    print( 'action_space: ', env.action_space )\n",
    "    \n",
    "    if _isObsContinuous :\n",
    "        print( 'Continuous state|observation space' )\n",
    "        print( 'obs-space-shape: ', env.observation_space.shape )\n",
    "    else :\n",
    "        print( 'Discrete state|observation space' )\n",
    "        print( 'obs-space-n-states: ', env.observation_space.n )\n",
    "        \n",
    "    if _isActContinuous :\n",
    "        print( 'Continuous action space' )\n",
    "        print( 'act-space-shape: ', env.action_space.shape )\n",
    "    else :\n",
    "        print( 'Discrete action space' )\n",
    "        print( 'act-space-n-actions: ', env.action_space.n )\n",
    "    \n",
    "    print( '-------------------------------------' )\n",
    "\n",
    "# Frozen-lake: discrete state-space, discrete action-space\n",
    "env = runner( 'FrozenLake-v0' )\n",
    "# Lunar-lander: continuous state-space, discrete action-space\n",
    "env = runner( 'LunarLander-v2' )\n",
    "# Pendulum: continuous state-space, continuous action-space\n",
    "env = runner( 'Pendulum-v0' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple rl-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make( 'FrozenLake-v0', is_slippery = False )\n",
    "# env = gym.make( 'LunarLander-v2' )\n",
    "# env = gym.make( 'Pendulum-v0' )\n",
    "# env = gym.make( 'Humanoid-v2' )\n",
    "\n",
    "# env.seed( 0 )\n",
    "\n",
    "# reset environment and grab state from initial distribution\n",
    "s = env.reset()\n",
    "\n",
    "aGamma = 0.99    # discount factor\n",
    "aScore = 0.      # score obtained in the whole episode\n",
    "aReturn = 0.     # Return 'G' from starting step\n",
    "aTrajectory = [] # trajectory taken by the agent [(s,a)]\n",
    "\n",
    "def random_policy( s ) :\n",
    "    global env\n",
    "    return env.action_space.sample()\n",
    "\n",
    "def naive_discrete_policy( s ) :\n",
    "    return 0\n",
    "\n",
    "def naive_continuous_policy( s ) :\n",
    "    global env\n",
    "    return np.zeros( *env.action_space.shape )\n",
    "\n",
    "# policy_fn = random_policy\n",
    "policy_fn = naive_discrete_policy if isinstance( env.action_space, gym.spaces.Discrete ) \\\n",
    "                                  else naive_continuous_policy\n",
    "\n",
    "# loop for a whole episode\n",
    "for i in tqdm( range( env.spec.max_episode_steps ) ) :\n",
    "    a = policy_fn( s )\n",
    "    snext, r, done, _ = env.step( a )\n",
    "    env.render()\n",
    "    \n",
    "    # some book-keeping\n",
    "    aTrajectory.append( (s, a) )\n",
    "    aScore += r\n",
    "    aReturn += r * ( aGamma ** i )\n",
    "    s = snext\n",
    "    \n",
    "    if done :\n",
    "        break\n",
    "        \n",
    "print( 'score: ', aScore  )\n",
    "print( 'return: ', aReturn )\n",
    "print( 'trajectory: ', aTrajectory )\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Gym environment wrappers\n",
    "\n",
    "Most of the publicly available baselines (like [this](https://github.com/hill-a/stable-baselines) one) require a gym-like interface to be used. So, if you want to test an off-the-shelf implementation of an rl-algorithm, the it's a good idea to wrap whatever environment you're currently working on with a gym-like interface.\n",
    "\n",
    "Also, this interface is kind of standard, and other researchers that might want to try to run their own experiments might have it easier if your environment is nicely wrapped and ready to use with their own baselines. So, let's see how we can kind of standarize this using a gym-like interface.\n",
    "\n",
    "### Just implement the gym.Env interface\n",
    "\n",
    "All there's to do is to implement the \"abstract\" methods of the gym.Env class, and also define your observation and action space. Below there's a snippet of what to implement:\n",
    "\n",
    "```python\n",
    "\n",
    "class MyEnv( gym.Env ) :\n",
    "    \n",
    "    def __init__( self, SOME-SEXY-ARGUMENTS ) :\n",
    "        super( MyEnv, self ).__init__()\n",
    "        \n",
    "        # ... initialize your own stuff ...\n",
    "        \n",
    "        # define your observation space\n",
    "        self.observation_space = gym.spaces.Discrete( N-STATES ) # in case of a discrete state-space\n",
    "        self.observation_space = gym.spaces.Continuous( STATES-DIM ) # in case of a continuous state-space\n",
    "        \n",
    "        # define your action space\n",
    "        self.action_space = gym.spaces.Discrete( N-ACTIONS ) # in case of discrete action-space\n",
    "        self.action_space = gym.spaces.Continuous( ACTIONS-DIM ) # in case of Continuous action-space\n",
    "        \n",
    "        # ... some more stuff ...\n",
    "        \n",
    "    def reset( self ) :\n",
    "        # > define here your own initial state|observation distribution\n",
    "        # > and return a sample from it\n",
    "        return INITIAL-STATE\n",
    "    \n",
    "    def seed( self ) :\n",
    "        # > seed your own generators here\n",
    "        # e.g. torch.manual_seed(0), np.random.seed(20)\n",
    "    \n",
    "    def step( self, action ) :\n",
    "        # > define the dynamics of your environment\n",
    "        # > use the action given to take a step in the environment\n",
    "        # > return the next state, reward, a termination flag, and some other info (dict, ...)\n",
    "        \n",
    "        return NEXT-STATE, REWARD, TERMINATION-FLAG, EXTRA-INFO\n",
    "    \n",
    "    def render( self ) :\n",
    "        # > use the visualizer that comes with your environment and render in the supported form\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.gridworld.environment import GridWorldEnv\n",
    "GridWorldEnv??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.mlagents.environment import UnityEnvWrapper\n",
    "UnityEnvWrapper??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## 3. Using other environments-->\n",
    "\n",
    "<!--Finally, just for the heck of it, let's check some other environments that are available online, either from competitions or other benchmarks that you might end up using. This isn't at all a comprehensive list, and some\n",
    "of the benchmarks that I've picked are the ones I'm most interested of. Nevertheless, it's very likely that other environments not covered here will follow a similar interface to the one provided by gym.Env.-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
